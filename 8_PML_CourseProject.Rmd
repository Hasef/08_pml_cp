---
title: 'Practical Machine Learning, Course Project'
author: "Harald Feibel"
date: "Monday, July 24, 2017"
output:
  html_document: default
  pdf_document: default
---

# Executive Summary
Based on the paper
*[Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th  International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013](http://groupware.les.inf.puc-rio.br/har#ixzz4neSsFIWU)* the goal of this course project is to develop a model to predict the manner in which a sportsman resp. sportswoman performs his resp. her weight lifting exercises. The input for building the prediction model is a given training set with many thousands observations. The prediction model computed has to be applied to a given very samll testing set of only a view observations. Finally the predictions obtained this way had to be entered as answers into the Course Project Prediction Quiz.

# Columns preselection of given training and testing set
Given training and testing data sets are loaded into corresponding data frames using *read.csv*. Existing NAs, empty values and special values of kind "#DIV/0!" (recognized during a preinspecting phase of the data files themselves) shall all be interpreted as NA:


```{r}
testing <- read.csv ("./pml-testing.csv", na.strings=c("NA","","#DIV/0!"))
training <- read.csv ("./pml-training.csv", na.strings=c("NA","","#DIV/0!"))
```

The removal of needless columns is based on two approaches:

* with a total of `r dim(training)[1]`  rows in the training set all columns with over 90% (i.e. more than `r options(scipen=999); trunc(dim(training)[1]*0.9)`) NA occurences are choosen to be removed from the data sets since they are considered irrelevant
* columns 1 to 7, being a simple rownumber id, containing the person's names and dealing with time windows are considered NOT to be relevant for the prediction model we are looking for, too.

The column removal is applied to the given training as well as to given testing set:
```{r}
columnsKeepIndicator <- sapply(training, 
                               function(x) ifelse(sum(ifelse(is.na(x), 1, 0)) > trunc(dim(training)[1]*0.9), 
                                                  FALSE, 
                                                  TRUE))
library(data.table)
training <- setDT(training) [, columnsKeepIndicator, with=FALSE]
training <- training[,8:length(colnames(training))]
# apply to testing, too:
testing <- setDT(testing) [, columnsKeepIndicator, with=FALSE]
testing <- testing[,8:length(colnames(testing))]
```

We finish this columns removal step by showing the remaining columns that really are used during the subsequent prediction model building and by assuring that resulting columns names are the same in training and testing set (of course except *outcome* column *"classe"* being part of training set only):
```{r}
colnames(training)
all.equal(colnames(training)[1-length(training)-1], colnames(testing)[1-length(training)-1])
```

# Building the prediction model and determining its accuracy
This paragraph is about building a prediction model with a sufficient accuracy that finally will allow the prediction of the outcomes for given small testing set.

### Creating an *own* training set and an *own* testing set
To really judge a model against a test data set we need to know the TRUE outcome of the test observations. This can *not* be achieved using the original given 20 observations testing set. An appropriate way to cope with this is that we will split the given *training* set itself into an (*own* training) subset we will really use for building the model and into an (*own* testing) subset we will use for analyzing the accuracy of our model.
```{r, warning=FALSE, message=FALSE, cache=TRUE}
library(caret)
inMyTrain <- createDataPartition(y=training$classe, p=.75, list=FALSE)
myTrain <- training[inMyTrain]
myTest  <- training[-inMyTrain]
```

### Building the prediction model using *caret::train()*
Having struggled a lot with memory and runtime issues due to the large size of the training data set when using the *caret*'s package *train()* function  we are finally following the advice given in [Improving Performance of Random Forest in caret::train()](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md) and are applying the parallel processing paradigm recommended there.

The model fitting itself uses the caret's package train() function 

- including standard preprocessing ("center", "scale")
- with method "rf" for random forest
- using only 10 trees (ntree=10)
- and a cross validation approach ("cv") with 5 folds only.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
modFit <- train (classe ~., method="rf", ntree=10, 
                 preProcess=c("center", "scale"), 
                 trControl=trainControl(method="cv", number=5, allowParallel = TRUE), 
                 data=myTrain)
stopCluster(cluster)
registerDoSEQ()
```
### Judge the model based on predictions on our *own* testing set
The model obtained can be used to predict the values for the *own* testing set (note: the one extracted from the original training set).
The confusion matrix and accuracy metric w.r.t. this "own" testing set are as follows:
```{r, warning=FALSE, message=FALSE, cache=TRUE}
predictions <- predict(modFit, newdata = myTest)
cm <- confusionMatrix(predictions, myTest$classe)
print(cm$table)
print(cm$overall[1])
```
With an accuracy of `r round(cm$overall[1], 3)` w.r.t. applying our model to our *own* big testing set 
the out of sample error seems to be small enough for applying this model to the orginally given small testing set.

### Predicting the answers for the course project prediction quiz
The model will now be used to predict the outcome of the *original* small `r dim(testing)[1]`  observations testing set. 
The corresponding `r dim(testing)[1]` predicted values will be used as the answers for the course project prediction quiz: 
```{r, warning=FALSE, message=FALSE, cache=TRUE}
predictions4Quiz <- predict(modFit, newdata = testing)
print(predictions4Quiz)
```



